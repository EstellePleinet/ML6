{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Forecasting with a Prophet ðŸ“‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One piece of recent open software is facebook's `prophet`. This uses a model which is similar to a **generalised additive model** (GAM), a class of additive (linear) models with potentially non-linear components. \n",
    "\n",
    "The model is easily interpreted (as a sum of components) and simple to fit (parameters have a strightforward interpretation). But if the hypothesis of the parametric model are not respected, the fitted model may seriously underfit.\n",
    "\n",
    "As you are beginning to see time series forecasting can sometimes be as much an art as a science, and we know it takes quite some practice to become a good artist. The authors of the software had this in mind when they wrote it. One of their aims is to provide software which can be used easily by those who have a little timeseries knowledge. Let's see how this goes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**\n",
    "\n",
    "- https://peerj.com/preprints/3190/\n",
    "- https://www.youtube.com/watch?v=2XFro0nIHQM&list=PLjwX9KFWtvNnOc4HtsvaDf1XYG3O5bv5s&index=10\n",
    "- https://www.youtube.com/watch?v=95-HMzxsghY\n",
    "- https://www.youtube.com/watch?v=pOYAXv15r3A "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required librairies**\n",
    "- [ ] prophet\n",
    "- [ ] numpy\n",
    "- [ ] pandas\n",
    "- [ ] matplotlib\n",
    "- [ ] seaborn\n",
    "- [ ] pmdarima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install prophet I advise you to install a completely clean virtual environment with conda (you will find the instructions to do this [here](https://docs.conda.io/projects/conda/en/4.6.1/user-guide/tasks/manage-environments.html)), with numpy, pandas, matplotlib, seaborn and pmdarima.\n",
    "\n",
    "Then you can install prophet with: `conda install conda-forge::prophet`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pmdarima as pm\n",
    "from prophet import Prophet\n",
    "pd.plotting.register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1 - Airline data\n",
    "\n",
    "Let's setup the data as we had done in the previous parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.datasets import load_airpassengers\n",
    "\n",
    "def ts_train_test_split(data, split_date):\n",
    "    '''\n",
    "    Split time series into training and test data\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    data - pd.DataFrame - time series data.  Index expected as datatimeindex\n",
    "    split_date - the date on which to split the time series\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple (len=2) \n",
    "    0. pandas.DataFrame - training dataset\n",
    "    1. pandas.DataFrame - test dataset\n",
    "    '''\n",
    "    train = data.loc[data.index < split_date]\n",
    "    test = data.loc[data.index >= split_date]\n",
    "    return train, test\n",
    "\n",
    "START_DATE = '1949-01-01'\n",
    "airline = load_airpassengers(as_series=True)\n",
    "\n",
    "# There's no datetimeindex from the bundled dataset. So let's add one.\n",
    "airline.index= pd.date_range(START_DATE, \n",
    "                             periods=len(airline), \n",
    "                             freq='MS')\n",
    "\n",
    "airline_adj = airline / airline.index.days_in_month # Remove spurious features due to variations of months duration\n",
    "train, test = ts_train_test_split(airline_adj, '1960-01-01') # Split the dataset into train and test set\n",
    "train_log, test_log = np.log(train), np.log(test) # Apply the log transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting with prophet\n",
    "\n",
    "Fitting a basic model and making predictions is very simple with `prophet`. You simply need to create a Prophet object, and then call its `fit` method on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is expected in this data frame format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train).reset_index().rename(columns = {0:'y','index':'ds'})\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Prophet()\n",
    "model.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = model.make_future_dataframe(periods=12, \n",
    "                                     freq='MS', \n",
    "                                     include_history=True) \n",
    "\n",
    "# Create a dataframe with the prediction datetimes we want\n",
    "future.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = model.predict(future)\n",
    "y_pred = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].set_index('ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = model.plot(forecast)\n",
    "test.plot(style='.r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating this baseline model error on test set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    '''\n",
    "    MAPE\n",
    "\n",
    "    Parameters:\n",
    "    --------\n",
    "    y_true -- np.array actual observations from time series\n",
    "    y_pred -- the predictions to evaluate\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float, scalar value representing the MAPE (0-100)\n",
    "    '''\n",
    "    #y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(test.values , \n",
    "                               forecast['yhat'][-12:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the Prophet at least beats the naive model that we defined in the last notebook (from the unlogged data). Not too bad, given the small number of lines of code! But looking at the plot it is not fitting very well. We should use a cross-validation scheme to find a better fitting model (before evaluating it on the test set!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation with Prophet\n",
    "\n",
    "The `prophet` package also provides cross validation routines that perform adjustable rolling windows as we have seen previously in other cross validation routines specialized for time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet.diagnostics import cross_validation\n",
    "from prophet.diagnostics import performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "365.25*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Prophet()\n",
    "model.fit(train_df)\n",
    "df_cv = cross_validation(model, \n",
    "                         initial='1461 days', \n",
    "                         period='365 days', \n",
    "                         horizon = '365 days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for performance metrics and plotting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = performance_metrics(df_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_cross_validation_metric(df_cv, metric='mape')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above is the standard output plot of the CV from `prophet`. It is different to what we have looked at before. Each grey point represents a prediction, made on a particular month. Because we made predictions over several horizons each are plotted on the graph. At around horizon 30 for example we see each of the plots over the first month of each of the 6, 12 month validations. The blue line shows a rolling window of the mape scores 'averaged' over the cv predictions. More info [here](https://facebook.github.io/prophet/docs/diagnostics.html).\n",
    "\n",
    "We are normally interested in the mean and std of the MAPE scores found at each window. This can be calculated from the dataframe returned from the `cross_validation` function above, using the function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_performance_calc(df_cv):\n",
    "    \" function to replace prophet cv 'performance metrics' function\"\n",
    "    df_cv['mape'] = abs(df_cv['y'] - df_cv['yhat']) /df_cv['y'] # make column with individual abs normalised values\n",
    "    results = df_cv.groupby(['cutoff']).mean() # finish mape calc for each of the folds (distinguised in 'cutoff' column of df_cv)\n",
    "    results = results.describe() # sumarise folds information\n",
    "    results = results['mape']# # return only mape column\n",
    "    return(results)\n",
    "\n",
    "mape_performance_calc(df_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find a MAPE of ~7.2% with std ~2%. This would have estimated the error we saw on the test set to a good degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters\n",
    "\n",
    "The model plot we saw earlier did not fit very well to the train data. We can change the model assumptions (like hyper-parameters) to change how well it is fitting the train data.\n",
    "\n",
    "`Prophet(\n",
    "    growth='linear',\n",
    "    changepoints=None,\n",
    "    n_changepoints=25,\n",
    "    changepoint_range=0.8,\n",
    "    yearly_seasonality='auto',\n",
    "    weekly_seasonality='auto',\n",
    "    daily_seasonality='auto',\n",
    "    holidays=None,\n",
    "    seasonality_mode='additive',\n",
    "    seasonality_prior_scale=10.0,\n",
    "    holidays_prior_scale=10.0,\n",
    "    changepoint_prior_scale=0.05,\n",
    "    mcmc_samples=0,\n",
    "    interval_width=0.8,\n",
    "    uncertainty_samples=1000,\n",
    "    stan_backend=None,\n",
    ")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "\n",
    "* Try changing some of these. [This](https://medium.com/data-science/implementing-facebook-prophet-efficiently-c241305405a3) blog is very useful in explaining the parameters and what they do.\n",
    "* Find at least one change which improves your model cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your solution here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional exercise - Energy data forecasting âš¡\n",
    "\n",
    "Forecasting energy demand is big business and very important in helping energy producers maintain the balance within the electrical grid. You can find forecasts of energy demands for France in realtime [here](https://www.rte-france.com/fr/eco2mix/eco2mix-consommation). The data was already downloaded in the `data/nats.csv` file.\n",
    "\n",
    "Your mission now is to imagine a useful use-case for an enterprise and use `Prophet` to make forecasts on the energy data you have already seen (in data viz module) for France! Feel free to choose :\n",
    "* the type of energy generation/consommation\n",
    "* france or regional,\n",
    "* frequency scale you wish to try!\n",
    "\n",
    "Try and consider a forecast horizon that seems useful for the frequency at which you are making your predictions e.g. predicting the hourly forecast for 12 months propbably cannot be used in any meaningful way - and is probably not going be very easy to forecast accurately.\n",
    "\n",
    "Consider that:\n",
    "* Reducing the scale of the data, i.e. to regional\n",
    "* Higher frequency predictions\n",
    "* longer periods of forecast\n",
    "\n",
    "...are likely to make it more difficult to make a good forecast.\n",
    "\n",
    "Consider also that you must pick the most suitable cross validation procedure for your problem. Consider the initial, step and horizon you will use carefully.\n",
    "\n",
    "Once you have made your model:\n",
    "1. Compare it to a naive model!! If it does not beat this then try reducing your forecast horizon to find at what scale you can make useful predictions.\n",
    "2. Try tuning the model in some way to improve performance (add holidays might be a good idea)\n",
    "3. If you have time compare your model to an ARIMA model. Why might there be differences in perfomance between the two approachecs to forecasting?\n",
    "\n",
    "(feel free to use a new notebook for this task!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
